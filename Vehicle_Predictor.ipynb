{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vehicle_Predictor.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMUQ6rMFD4ZUYet72OW8MTe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dharineeshramtp2000/Gradient-Boosting-Vehicle-Prediction-Silhouette-Measurements-/blob/master/Vehicle_Predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmeaUi3cXjJ6",
        "colab_type": "text"
      },
      "source": [
        "Import the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0gG7dpoXchH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zwp1EEG2XqUH",
        "colab_type": "text"
      },
      "source": [
        "Importing the Dataset. Here we use the dataset from [ML Data](https://www.mldata.io/datasets/).\n",
        "![](https://www.thestreet.com/.image/ar_3:2%2Cc_limit%2Ccs_srgb%2Cq_auto:good%2Cw_600/MTY4NjUxNDMwMjY4NjQyOTUx/14-gmc-sierra.png)\n",
        "---\n",
        "\n",
        "Here we are going to predict the vehicle type based on silhouette measurements\n",
        "\n",
        "The types of Vehicles are \n",
        "\n",
        "\n",
        "1.   Opel\n",
        "2.   saab\n",
        "3.   Bus\n",
        "4.   Van\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tifsPeCMYo2u",
        "colab_type": "code",
        "outputId": "546be0dd-2b04-4080-f1aa-f07e6f11cf8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "Dataset = pd.read_csv(\"vehicle_silhouette_weka_dataset.csv\")\n",
        "X = Dataset.iloc[:,:-1].values\n",
        "y = Dataset.iloc[:,-1].values\n",
        "Dataset.describe()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>compactness</th>\n",
              "      <th>circularity</th>\n",
              "      <th>distance_circularity</th>\n",
              "      <th>radius_ratio</th>\n",
              "      <th>pr_axis_aspect_ratio</th>\n",
              "      <th>max_length_aspect_ratio</th>\n",
              "      <th>scatter_ratio</th>\n",
              "      <th>elongatedness</th>\n",
              "      <th>pr_axis_rectangularity</th>\n",
              "      <th>max_length_rectangularity</th>\n",
              "      <th>scaled_variance_major_axis</th>\n",
              "      <th>scaled_variance_minor_axis</th>\n",
              "      <th>scaled_radius_gyration</th>\n",
              "      <th>skewness_major_axis</th>\n",
              "      <th>skewness_minor_axis</th>\n",
              "      <th>kurtosis_minor_axis</th>\n",
              "      <th>kurtosis_major_axis</th>\n",
              "      <th>hollows_ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>846.000000</td>\n",
              "      <td>846.000000</td>\n",
              "      <td>846.000000</td>\n",
              "      <td>846.000000</td>\n",
              "      <td>846.000000</td>\n",
              "      <td>846.000000</td>\n",
              "      <td>846.000000</td>\n",
              "      <td>846.000000</td>\n",
              "      <td>846.000000</td>\n",
              "      <td>846.000000</td>\n",
              "      <td>846.000000</td>\n",
              "      <td>846.000000</td>\n",
              "      <td>846.00000</td>\n",
              "      <td>846.000000</td>\n",
              "      <td>846.000000</td>\n",
              "      <td>846.000000</td>\n",
              "      <td>846.000000</td>\n",
              "      <td>846.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>93.678487</td>\n",
              "      <td>44.861702</td>\n",
              "      <td>82.088652</td>\n",
              "      <td>168.940898</td>\n",
              "      <td>61.693853</td>\n",
              "      <td>8.567376</td>\n",
              "      <td>168.839243</td>\n",
              "      <td>40.933806</td>\n",
              "      <td>20.582742</td>\n",
              "      <td>147.998818</td>\n",
              "      <td>188.625296</td>\n",
              "      <td>439.911348</td>\n",
              "      <td>174.70331</td>\n",
              "      <td>72.462175</td>\n",
              "      <td>6.377069</td>\n",
              "      <td>12.599291</td>\n",
              "      <td>188.932624</td>\n",
              "      <td>195.632388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>8.234474</td>\n",
              "      <td>6.169866</td>\n",
              "      <td>15.771533</td>\n",
              "      <td>33.472183</td>\n",
              "      <td>7.888251</td>\n",
              "      <td>4.601217</td>\n",
              "      <td>33.244978</td>\n",
              "      <td>7.811560</td>\n",
              "      <td>2.592138</td>\n",
              "      <td>14.515652</td>\n",
              "      <td>31.394837</td>\n",
              "      <td>176.692614</td>\n",
              "      <td>32.54649</td>\n",
              "      <td>7.486974</td>\n",
              "      <td>4.918353</td>\n",
              "      <td>8.931240</td>\n",
              "      <td>6.163949</td>\n",
              "      <td>7.438797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>73.000000</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>104.000000</td>\n",
              "      <td>47.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>118.000000</td>\n",
              "      <td>130.000000</td>\n",
              "      <td>184.000000</td>\n",
              "      <td>109.00000</td>\n",
              "      <td>59.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>176.000000</td>\n",
              "      <td>181.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>87.000000</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>141.000000</td>\n",
              "      <td>57.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>146.250000</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>137.000000</td>\n",
              "      <td>167.000000</td>\n",
              "      <td>318.250000</td>\n",
              "      <td>149.00000</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>184.000000</td>\n",
              "      <td>190.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>93.000000</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>167.000000</td>\n",
              "      <td>61.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>157.000000</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>146.000000</td>\n",
              "      <td>178.500000</td>\n",
              "      <td>364.000000</td>\n",
              "      <td>173.00000</td>\n",
              "      <td>71.500000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>188.000000</td>\n",
              "      <td>197.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>100.000000</td>\n",
              "      <td>49.000000</td>\n",
              "      <td>98.000000</td>\n",
              "      <td>195.000000</td>\n",
              "      <td>65.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>198.000000</td>\n",
              "      <td>46.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>159.000000</td>\n",
              "      <td>217.000000</td>\n",
              "      <td>587.000000</td>\n",
              "      <td>198.00000</td>\n",
              "      <td>75.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>193.000000</td>\n",
              "      <td>201.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>119.000000</td>\n",
              "      <td>59.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>333.000000</td>\n",
              "      <td>138.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>265.000000</td>\n",
              "      <td>61.000000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>188.000000</td>\n",
              "      <td>320.000000</td>\n",
              "      <td>1018.000000</td>\n",
              "      <td>268.00000</td>\n",
              "      <td>135.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>206.000000</td>\n",
              "      <td>211.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       compactness  circularity  ...  kurtosis_major_axis  hollows_ratio\n",
              "count   846.000000   846.000000  ...           846.000000     846.000000\n",
              "mean     93.678487    44.861702  ...           188.932624     195.632388\n",
              "std       8.234474     6.169866  ...             6.163949       7.438797\n",
              "min      73.000000    33.000000  ...           176.000000     181.000000\n",
              "25%      87.000000    40.000000  ...           184.000000     190.250000\n",
              "50%      93.000000    44.000000  ...           188.000000     197.000000\n",
              "75%     100.000000    49.000000  ...           193.000000     201.000000\n",
              "max     119.000000    59.000000  ...           206.000000     211.000000\n",
              "\n",
              "[8 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMX-6lXTZsod",
        "colab_type": "text"
      },
      "source": [
        "Lets Feature Scale the independent variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erXFDhgeNSRV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_X = StandardScaler()\n",
        "X = sc_X.fit_transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2CaoxRcZeMd",
        "colab_type": "text"
      },
      "source": [
        "Splitting the dataset into training and testing and performing a good shuffle of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP83grDqZlOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 0.3, random_state =42, shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E82SIY2PZmNj",
        "colab_type": "text"
      },
      "source": [
        "Now lets define our Gradient Boost Classifier.\n",
        "We know how trees work. We use these attributes in order to make the splits and finally make the prediction using the leaf values of the tree.\n",
        "Instead of using normal trees, lets define it using boosting algorithm(**here Gradient Boost**)\n",
        "\n",
        "![](https://www.frontiersin.org/files/Articles/284242/fnagi-09-00329-HTML/image_t/fnagi-09-00329-g001.gif)\n",
        "\n",
        "Feel free to use the [Documnetaton for Gradient Boost Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9bLh52NbAxk",
        "colab_type": "code",
        "outputId": "2d97c34a-609d-43ca-fc8d-cad10813b48e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "classifier = GradientBoostingClassifier(loss = 'deviance', learning_rate = 0.1, n_estimators =20,min_samples_split=3,max_depth = 3,max_features = 'sqrt',random_state=43)\n",
        "classifier.fit(X_train, y_train)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
              "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
              "                           max_features='sqrt', max_leaf_nodes=None,\n",
              "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                           min_samples_leaf=1, min_samples_split=3,\n",
              "                           min_weight_fraction_leaf=0.0, n_estimators=20,\n",
              "                           n_iter_no_change=None, presort='deprecated',\n",
              "                           random_state=43, subsample=1.0, tol=0.0001,\n",
              "                           validation_fraction=0.1, verbose=0,\n",
              "                           warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2j8E0L4b78d",
        "colab_type": "text"
      },
      "source": [
        "Now lets predict with our test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fMBI0Utb_rW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = classifier.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNpmUnI7N4m6",
        "colab_type": "text"
      },
      "source": [
        "Now lets evaluate the accuracy , F1 score and the Confusion Matrix in order to check how well our model is classifying new data\n",
        "\n",
        "---\n",
        "\n",
        "First lets see how model works with our tarining examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KYRW9ybOTBa",
        "colab_type": "code",
        "outputId": "ea6d85cc-4339-40d3-fc66-4c8c962bc874",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
        "acc_train = accuracy_score(y_train, classifier.predict(X_train))\n",
        "f1_train = f1_score(y_train, classifier.predict(X_train), average= 'weighted')\n",
        "\n",
        "print(\"Traing set results\")\n",
        "print(\"ACCURACY ---------------------->\",acc_train)\n",
        "print(\"F1 SCORE ---------------------->\",f1_train)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traing set results\n",
            "ACCURACY ----------------------> 0.875\n",
            "F1 SCORE ----------------------> 0.8709885455631511\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1M_725gOfEp",
        "colab_type": "text"
      },
      "source": [
        "Now lets see how well is our model. So now lets evaluate with our test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot3d3ndRN27r",
        "colab_type": "code",
        "outputId": "ffae7241-e6fe-47fe-a756-dfea06dcb1ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "acc_test = accuracy_score(y_test, y_pred)\n",
        "f1_test = f1_score(y_test, y_pred, average= 'weighted')\n",
        "\n",
        "print(\"Test set results\")\n",
        "print(\"ACCURACY ---------------------->\",acc_test)\n",
        "print(\"F1 SCORE ---------------------->\",f1_test)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set results\n",
            "ACCURACY ----------------------> 0.7598425196850394\n",
            "F1 SCORE ----------------------> 0.7456802239154305\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX084s0rP4NE",
        "colab_type": "text"
      },
      "source": [
        "Not bad, \n",
        "\n",
        "Now lets visualize the Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBNvtJoRP6N8",
        "colab_type": "code",
        "outputId": "68171193-b66e-45c6-c0fe-5c08c9c8ce3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "cm = confusion_matrix(y_test,y_pred)\n",
        "print(cm)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[76  0  0  2]\n",
            " [ 3 30 11  3]\n",
            " [ 8 25 31  7]\n",
            " [ 0  1  1 56]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_wgolayP-Yp",
        "colab_type": "text"
      },
      "source": [
        "Our Foresting algorithm with Bosting technique(here Gradient Boost) has done well in predicting our Train as well as Test set. So it has made the **appropriate fitting**\n",
        "\n",
        "We have obtained this great accuracy for this challenging Dataset. If this dataset was fed in a normal Random Forest model, then it would have given a higher variance problem. But in Gradient Boosting, we were able to achieve this good accuracy and also not higher variance.\n"
      ]
    }
  ]
}